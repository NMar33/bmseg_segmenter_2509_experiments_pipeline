# scripts/make_reports.py

"""
Script to collect evaluation results and generate summary tables for the paper.

This script scans a directory for JSON report files generated by `evaluate.py`,
groups them by model type (extracted from the filename), and calculates
the mean and standard deviation of metrics across all random seeds.

The final output is a Markdown-formatted table, ready to be copied into
a research paper or report.
"""
import argparse
from pathlib import Path
import json
import pandas as pd

def main():
    parser = argparse.ArgumentParser(description="Generate result tables from evaluation JSONs.")
    parser.add_argument("--reports_dir", default="./reports", help="Directory containing evaluation JSON report files.")
    parser.add_argument("--filter_prefix", default="", help="Optional prefix to filter JSON files (e.g., 'isbi_M1').")
    args = parser.parse_args()

    reports_dir = Path(args.reports_dir)
    if not reports_dir.is_dir():
        raise FileNotFoundError(f"Reports directory not found: {reports_dir}")

    # --- 1. Collect and Load Data ---
    all_results = []
    json_files = sorted(reports_dir.glob(f"{args.filter_prefix}*.json"))
    
    if not json_files:
        print(f"Warning: No JSON files found with prefix '{args.filter_prefix}' in '{reports_dir}'")
        return

    print(f"Found {len(json_files)} report files to process.")

    for file_path in json_files:
        with open(file_path, 'r') as f:
            data = json.load(f)
            # Извлекаем имя модели из `config_file` внутри JSON для надежности
            config_stem = Path(data['config_file']).stem
            data['model_name'] = config_stem
            all_results.append(data)
    
    df = pd.DataFrame(all_results)
    
    # --- 2. Aggregate and Format Results ---
    # DEV: Мы группируем по имени модели и считаем среднее и стандартное отклонение
    # по всем сидам. Это дает нам статистически значимые результаты.
    
    metric_cols = sorted([col for col in df.columns if col.endswith('_mean')])
    if not metric_cols:
        print("No metric columns found in the reports. Ensure `evaluate.py` saves columns ending in '_mean'.")
        return

    summary = df.groupby('model_name')[metric_cols].agg(['mean', 'std'])
    
    # Форматируем в красивую строку "mean ± std"
    for col in metric_cols:
        summary[col, 'formatted'] = summary.apply(
            lambda row: f"{row[col, 'mean']:.4f} ± {row[col, 'std']:.4f}",
            axis=1
        )
    
    # --- 3. Print Markdown Table ---
    display_cols = [(col, 'formatted') for col in metric_cols]
    final_table = summary[display_cols]
    final_table.columns = [col[0].replace('_mean', '').upper() for col in display_cols]

    print(f"\n--- Summary Table for prefix '{args.filter_prefix}' ---")
    print(final_table.to_markdown(index=True))


if __name__ == "__main__":
    main()