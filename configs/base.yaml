# configs/base.yaml

# ====================================================================
# BASE CONFIGURATION
# This file is the master template for all experiments.
# Experimental configs (e.g., in `configs/exp/`) should inherit from
# and override the values defined here.
# ====================================================================

# A list of random seeds to run the experiment on.
# The `run_experiment.py` orchestrator will iterate through this list,
# creating a separate run for each seed.
run_seeds: [42, 11] #[42, 1337, 2023]


# --- DATA CONFIGURATION ---
data:
  # Paths to data directories. These MUST be overridden in experimental configs.
  prepared_data_root: "../prepared_data/placeholder" # Path to the canonical dataset
  interim_data_root: "./interim_data/placeholder"      # Path to store tiles and splits

  # --- SPLIT GENERATION STRATEGY ---
  # This section is the source of truth for how to create train/val/test splits.
  # It is executed by the `generate-splits` script for each seed in `run_seeds`.
  split_generation:
    # `source_splits` defines which folders from `prepared_data_root` to use.
    # The value for each key (`train`, `val`, `test`) can be:
    #   - A list of strings: `["folder1", "folder2"]` -> Use images from these specific folders.
    #   - A float: `0.15` -> Randomly sample this fraction from a source set.
    source_splits:
      # `train`: Required. List of folders for the initial training pool.
      train: ["train"]

      # `val`: Defines the validation set.
      #   - If a list (e.g., `["val"]`), uses a fixed set from that folder.
      #   - If a float (e.g., `0.15`), carves out a random fraction FROM THE `train` SET.
      val: 0.15 

      # `test`: Defines the test set.
      #   - If a list (e.g., `["test"]`), uses a fixed set from that folder.
      #   - If a float (e.g., `0.2`), carves out a random fraction.
      #     THE SOURCE FOR CARVING IS ALWAYS THE VALIDATION SET (`val`).
      #     This ensures the train set is touched as little as possible.
      #     An error will be raised if `val` is not defined when `test` is a float.
      test: ["test"]
  
  # Tiling parameters for `preprocess-tiles` script.
  tiling:
    tile_size: 512
    overlap: 64


  # --- НОВЫЙ БЛОК: ВСТАВИТЬ ЭТОТ КОД ---
  # --- Mask Processing Configuration ---
  # This section defines how raw mask files (0-255) are converted into
  # the binary {0, 1} format required for training. This is a CRITICAL
  # parameter that defines what the model considers the "target" class.
  # Incorrectly setting this will lead to the model learning the wrong thing.
  mask_processing:
    # `binarize_mode` determines which pixel values become the target class (1).
    #
    # Available modes:
    # - "nonzero": Any pixel value > 127 becomes 1. Correct for masks where
    #              the object of interest is white (255) and the background is black (0).
    #              This is the most common case.
    #
    # - "zero": Any pixel value < 128 becomes 1. Correct for inverted masks where
    #           the object of interest (e.g., boundaries) is black (0) and the
    #           background is white (255).
    #
    # - "ignore_zero": Any pixel value > 0 becomes 1. Correct for instance masks
    #                  where the background is 0, and each object has a unique
    #                  ID (1, 2, 3, ...). This treats all objects as a single class.
    binarize_mode: "zero" #"nonzero" # Set a safe and common default.
  
  # --- AUGMENTATION CONFIGURATION ---
  # This section controls the augmentations applied during training.
  # Each key corresponds to an augmentation from the Albumentations library.
  # To disable an augmentation, set its probability `p` to 0.0.
  augmentations:
    # --- IMPORTANT ---
    # Photometric augmentations (RandomBrightnessContrast, RandomGamma, GaussNoise, etc.)
    # are NOT compatible with multi-channel feature banks in the current implementation.
    # They should ONLY be used when `data.feature_bank.use` is `false`.
    # Using them with feature banks will lead to inconsistent data.
    
    # Geometric augmentations (safe for multi-channel)
    HorizontalFlip: { p: 0.5 }
    VerticalFlip: { p: 0.5 }
    RandomRotate90: { p: 0.5 }
    ShiftScaleRotate: { p: 0.5, shift_limit: 0.0625, scale_limit: 0.1, rotate_limit: 15 }
    GaussianBlur: { p: 0.0, blur_limit: [3, 7] } # Disabled by default

    # Photometric augmentations (use only with single-channel data)
    RandomBrightnessContrast: { p: 0.0, brightness_limit: 0.2, contrast_limit: 0.2 }
    RandomGamma: { p: 0.0, gamma_limit: [80, 120] }
    GaussNoise: { p: 0.0, var_limit: [10.0, 50.0] }
  
  # Feature Bank parameters for `preprocess-tiles` script.
  feature_bank:
    use: false
    base_norm: zscore # "zscore" or "none"
    channels:
      - raw
      - clahe
      - scharr
      - laplacian
      - log_sigma1
      - log_sigma2
      - frangi
  
  save_float16: true

# --- MODEL CONFIGURATION ---
model:
  name: "unet"
  encoder: "timm-efficientnet-b3"
  encoder_weights: "imagenet" # `null` for training from scratch
  classes: 1

  adapter:
    use: true
    out_channels: 3
    init: "xavier" # Initialization method for the Conv2d weights.

    # --- NEW: Soft Initialization Weights ---
    # This section allows applying a multiplier to the initial weights of
    # specific input channels of the adapter, right after initialization.
    # It helps to prioritize certain channels (like 'raw') at the start of training.
    initial_channel_weights:
      # `default` is the multiplier applied to any channel NOT explicitly listed below.
      # A small value "dampens" the initial impact of feature channels.
      default: 0.01
      
      # Set the multiplier for the 'raw' channel to 1.0 to keep its
      # original initialized weights, effectively giving it priority.
      raw: 1.0


# --- TRAINING CONFIGURATION ---
train:
  epochs: 50
  batch_size: 8
  num_workers: 2
  amp: true
  
  # --- Fine-Tuning Strategy ---
  # This section controls how a pre-trained model is fine-tuned.
  # It is typically used for models with an adapter (M2, M3).
  fine_tuning:
    # Number of initial epochs to keep the encoder weights frozen.
    # During this phase, only the head (adapter, decoder) is trained.
    # Set to 0 to train the entire model from the start.
    freeze_encoder_epochs: 10

  # --- Optimizer Configuration ---
  optimizer:
    name: "adamw"
    # `lr` is the main learning rate, used for the model's head (decoder, adapter).
    lr: 3.0e-4
    # `encoder_lr` is the specific learning rate for the encoder backbone during fine-tuning.
    # It is recommended to use a smaller value than `lr`.
    # If `null`, it will default to `lr`.
    encoder_lr: 3.0e-5
    weight_decay: 1.0e-4
  
  # --- Scheduler Configuration ---
  scheduler:
    # `warmup_epochs`: Number of epochs for the linear learning rate warmup.
    # The LR will increase from a very small value to the target LR over this period.
    # This is highly recommended to stabilize training at the beginning.
    # Set to 0 to disable.
    warmup_epochs: 10
    # `main_scheduler_type` is the schedule to use *after* the warmup phase.
    main_scheduler_type: "cosine"

  # --- Validation Visualization ---
  # Configures visualization of validation samples during training.
  validation_visualization:
    enabled: true        # Master switch to log validation previews to MLflow.
    num_samples: 4       # How many fixed validation samples to show per epoch.
    # DEV: Мы можем переиспользовать стили из секции `eval.visualization`,
    # но лучше определить их здесь явно для независимости модулей.
    style:
      figsize_per_panel: [5, 5]
      dpi: 100 # Меньше DPI для более "легких" артефактов
      title_fontsize: 10
      suptitle_fontsize: 14
      # Используем те же цвета, что и в `eval`
      gt_color_rgb: [0, 170, 0]
      pred_color_rgb: [227, 27, 27]
      overlay_alpha: 0.4

# # --- TRAINING CONFIGURATION ---
# train:
#   epochs: 3
#   batch_size: 8
#   num_workers: 2
#   amp: true
  
#   # --- НОВЫЙ БЛОК ЗДЕСЬ ---
#   # Configures visualization of validation samples during training.
#   validation_visualization:
#     enabled: true        # Master switch to log validation previews to MLflow.
#     num_samples: 4       # How many fixed validation samples to show per epoch.
#     # DEV: Мы можем переиспользовать стили из секции `eval.visualization`,
#     # но лучше определить их здесь явно для независимости модулей.
#     style:
#       figsize_per_panel: [5, 5]
#       dpi: 100 # Меньше DPI для более "легких" артефактов
#       title_fontsize: 10
#       suptitle_fontsize: 14
#       # Используем те же цвета, что и в `eval`
#       gt_color_rgb: [0, 170, 0]
#       pred_color_rgb: [227, 27, 27]
#       overlay_alpha: 0.4
  
#   optimizer:
#     name: "adamw"
#     lr: 3.0e-4
#     weight_decay: 1.0e-4
  
#   scheduler:
#     name: "cosine"
#     params: {}

# --- LOSS FUNCTION CONFIGURATION ---
loss:
  name: "dice_bce"
  params:
    dice_weight: 0.5
    bce_weight: 0.5
    pos_weight: 1.0

# # --- EVALUATION CONFIGURATION ---
# eval:
#   batch_size: 16
#   metrics: ["dice", "iou", "bf1"]
  
#   # --- NEW: ADVANCED VISUALIZATION SECTION ---
#   visualization:
#     enabled: true  # Master switch to enable/disable all visualization generation.
    
#     # --- Selection Strategy ---
#     # Selects which images from the test set to visualize.
#     primary_metric: "dice" # Metric used to sort for 'best' and 'worst'.
#     num_best: 1            # Number of samples with the highest metric score to visualize.
#     num_worst: 1           # Number of samples with the lowest metric score to visualize.
#     num_random: 1          # Number of random samples for an unbiased view.

#     # --- Feature Bank Visualization ---
#     # If true, for each selected image, an additional plot showing all
#     # input filter channels will be generated. This is only active if
#     # the model was trained with `feature_bank.use: true`.
#     visualize_feature_bank: true

#     # --- Styling for Visualization Plots ---
#     style:
#       figsize_per_panel: [6, 6] # (width, height) for each panel in the grid
#       dpi: 150
#       title_fontsize: 12
#       # Colormap for filter visualizations (if visualize_feature_bank is true)
#       colormap: "viridis" 
#       # Overlay settings for the main comparison plot
#       overlay_alpha: 0.4
#       # Colors are in RGB format
#       gt_color_rgb: [0, 255, 0]      # Green for Ground Truth
#       pred_color_rgb: [227, 27, 27] # Red for Prediction

# --- EVALUATION CONFIGURATION ---
eval:
  batch_size: 16
  metrics: ["dice", "iou", "bf1", "pixel_error"]
  
  visualization:
    enabled: true
    
    # --- Selection Strategy ---
    primary_metric: "dice"
    num_best: 1
    num_worst: 1
    num_random: 1

    # --- Feature Bank Visualization ---
    visualize_feature_bank: true

    # --- Styling for Visualization Plots ---
    style:
      # General plot settings
      suptitle_fontsize: 16
      title_fontsize: 12
      dpi: 150
      
      # Layout settings
      # For the main comparison plot, we'll use a 2x3 grid.
      comparison_layout_cols: 3
      figsize_per_panel: [6, 6] # (width, height) for each panel in the grid
      
      # Colormap for filter visualizations and heatmaps
      colormap: "viridis"
      
      # Colors are in RGB format [R, G, B]
      # --- Overlay settings ---
      overlay_alpha: 0.4
      gt_overlay_color_rgb: [0, 170, 0]   # Darker Green for Ground Truth
      pred_overlay_color_rgb: [227, 27, 27] # Red for Prediction
      
      # --- Error Map Colors ---
      error_map_tp_color_rgb: [80, 80, 80]     # Gray for True Positive (correctly found)
      error_map_fp_color_rgb: [255, 0, 0]     # Red for False Positive (hallucination)
      error_map_fn_color_rgb: [0, 0, 255]     # Blue for False Negative (missed object)
      
      # --- Info Panel Settings ---
      info_panel_bg_color_rgb: [20, 20, 20] # Dark background
      info_panel_font_color_bgr: [255, 255, 255] # White font (in BGR for OpenCV)
      info_panel_font_scale: 0.8
      info_panel_font_thickness: 1


# --- LOGGING CONFIGURATION ---
logging:
  backend: "mlflow"
  experiment_name: "domain_shift_segmentation"
  artifact_uri: "./mlruns"